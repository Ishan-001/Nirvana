{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport numpy as np\nimport sklearn\nimport utils\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"numpy\")\nfrom numpy.random import permutation, randint\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom statistics import mode\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import confusion_matrix\nimport pickle\n\ndata = np.loadtxt('features_others_low_energy.npy',delimiter=',',dtype=float)\ndata1 = np.loadtxt('mfcc_gaussian.npy', delimiter=',',dtype=float)\ndata = np.append(data1, data, 1)\nlabels = np.loadtxt('labels_others_low_energy.npy',delimiter=',',dtype=float)\nlabels.astype(int)\ngenres = [1,5,6,7]\n#Create new data and labels that have genres : Classical, Jazz, Metal, Pop\nnew_data = data[np.logical_or.reduce([labels == x for x in genres])]\ndata = new_data\nnew_labels = labels[np.logical_or.reduce([labels == x for x in genres])]\nlabels = new_labels\n\n#kmeans clustering\nkmeans = KMeans(n_clusters=len(genres), random_state=0).fit(data)#, train_label)\nlabelmap = np.zeros((10,len(genres)))\nfor i in range(0,len(kmeans.labels_)):\n    labelmap[int(labels[i])][kmeans.labels_[i]] += 1\naccuracy_kmeans = (sum(labelmap.max(axis=0))/len(labels))*100\nprint 'k-Means : ', accuracy_kmeans\n\n# Randomly shuffle the index of nba.\ninsertCounter = 0\nfor labelType in genres:\n    genreData = data[np.logical_or.reduce([labels == labelType])]\n    genreLabels = labels[np.logical_or.reduce([labels == labelType])]\n    randomize = np.arange(len(genreData))\n    np.random.shuffle(randomize)\n    genreData = genreData[randomize]\n    genreLabels = genreLabels[randomize]\n    testGenreCutoff = int(math.floor(len(genreData)/3))\n    if insertCounter > 0:\n        test_data = np.append(test_data,genreData[0:testGenreCutoff],axis=0)\n        test_label = np.append(test_label, genreLabels[0:testGenreCutoff],axis=0)\n        train_data = np.append(train_data, genreData[testGenreCutoff:],axis=0)\n        train_label = np.append(train_label, genreLabels[testGenreCutoff:],axis=0)\n    else:\n        test_data = genreData[0:testGenreCutoff]\n        test_label = genreLabels[0:testGenreCutoff]\n        train_data = genreData[testGenreCutoff:]\n        train_label = genreLabels[testGenreCutoff:]\n        insertCounter += 1\n\n\nrandomize = np.arange(len(test_data))\nnp.random.shuffle(randomize)\ntest_data = test_data[randomize]\ntest_label = test_label[randomize]\n\nrandomize = np.arange(len(train_data))\nnp.random.shuffle(randomize)\ntrain_data = train_data[randomize]\ntrain_label = train_label[randomize]\n\n# Set a cutoff for how many items we want in the test set (in this case 1/3 of the items)\n#test_cutoff = int(math.floor(len(data)/3))\n# Generate the test set by taking the first 1/3 of the randomly shuffled indices.\n#test_data = data[0:test_cutoff]\n#Transform to scalar\n#scalar = sklearn.preprocessing.StandardScaler()\n#test_data = scalar.fit_transform(test_data)\n\n#test_label = labels[0:test_cutoff]\n# Generate the train set with the rest of the data.\n#train_data = data[test_cutoff:]\n#train_label = labels[test_cutoff:]\n#Transform to scalar\n#train_data = scalar.fit_transform(train_data)\n\n#KNN Classifier\nfor k in range(1,11):\n    neigh = KNeighborsClassifier(n_neighbors=k, algorithm='auto', metric='minkowski', p=2)\n    neigh.fit(train_data,train_label)\n    predictions_knn = neigh.predict(test_data)\n    pickle.dump(neigh, open('knn_4Genre.model', 'wb'))\nprint 'KNN : ', neigh.score(test_data, test_label) * 100#, ' k = ', 10\ncnf_knn = confusion_matrix(test_label, predictions_knn)\n\n#SVM Classifier\nsvc = svm.LinearSVC(random_state=0)\nsvc = OneVsRestClassifier(svc)\nsvm = CalibratedClassifierCV(svc, cv=10)\nsvm.fit(train_data,train_label)\npredictions_svm = svm.predict(test_data)\npickle.dump(svm, open('svm_4Genre.model', 'wb'))\nprint 'SVM : ', svm.score(test_data,test_label)*100\ncnf_svm = confusion_matrix(test_label, predictions_svm)\n\n#Decision Tree Classifier\ndt = tree.DecisionTreeClassifier()\ndt = dt.fit(train_data, train_label)\npredictions_decision = dt.predict(test_data)\npickle.dump(dt, open('DT_4Genre.model', 'wb'))\nprint 'Decision Tree : ', dt.score(test_data,test_label)*100\ncnf_dt = confusion_matrix(test_label, predictions_decision)\n\n#Neural Network Classifier\nann = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,), random_state=1,activation='tanh')\nann.fit(train_data, train_label)\npredictions_neural = ann.predict(test_data)\npickle.dump(ann, open('MLP_4Genre.model', 'wb'))\nprint 'Neural Network : ', ann.score(test_data,test_label)*100\ncnf_ann = confusion_matrix(test_label, predictions_neural)\n\n#Naive Bayes\nnb = GaussianNB()\nnb.fit(train_data,train_label)\npredictions_naive = nb.predict(test_data)\npickle.dump(nb, open('GausianNaiveBayes_4Genre.model', 'wb'))\nprint 'Naive Bayes : ', nb.score(test_data,test_label)*100\ncnf_nb = confusion_matrix(test_label, predictions_naive)\n\n#Logistic Regression\nlogistic = LogisticRegression(multi_class='ovr')\nlogistic.fit(train_data,train_label)\npredictions_lr = logistic.predict(test_data)\npickle.dump(nb, open('logistic_4Genre.model', 'wb'))\nprint 'LR : ', logistic.score(test_data,test_label)*100\ncnf_lr = confusion_matrix(test_label, predictions_lr)\n\n#Ensemble Voting Classifier\neclf3 = VotingClassifier(estimators=[\n       ('knn', neigh), ('svm', svm), ('dt', dt), ('ann', ann), ('nb', nb), ('lr', logistic)],\n       voting='soft', weights=[1,1,1,1,1,1])\neclf3 = eclf3.fit(train_data, train_label)\npredictions_ensemble = eclf3.predict(test_data)\npickle.dump(nb, open('ensemble_4Genre.model', 'wb'))\nprint 'Ensemble : ', eclf3.score(test_data,test_label)*100\ncnf_ensemble = confusion_matrix(test_label, predictions_ensemble)\n\n#Visualize the audio samples as t-SNE\nX_tsne = TSNE(learning_rate=1000,n_components=2).fit_transform(new_data)\nplt.scatter(X_tsne[:,0], X_tsne[:,1], c=new_labels,label=genres, cmap=plt.cm.get_cmap(\"jet\", 10))\nplt.title('T-SNE Plot for all audio files')\nplt.show()\n\nutils.plot_confusion_matrix(cnf_knn, title='Confusion Matrix for K-NN', classes=genres)\nutils.plot_confusion_matrix(cnf_svm, title='Confusion Matrix for SVM', classes=genres)\nutils.plot_confusion_matrix(cnf_dt, title='Confusion Matrix for Decision Tree', classes=genres)\nutils.plot_confusion_matrix(cnf_ann, title='Confusion Matrix for Artificial Neural Network', classes=genres)\nutils.plot_confusion_matrix(cnf_nb, title='Confusion Matrix for Naive Bayes', classes=genres)\nutils.plot_confusion_matrix(cnf_lr, title='Confusion Matrix for Logistic Regression', classes=genres)\nutils.plot_confusion_matrix(cnf_ensemble, title='Confusion Matrix for Ensemble classifier', classes=genres)\n","metadata":{},"execution_count":null,"outputs":[]}]}